"""
ë©”ëª¨ë¦¬ ìµœì í™” ê´€ë ¨ ìœ í‹¸ë¦¬í‹°
"""

import gc
import psutil
import os


def monitor_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (ìµœì í™”ëœ ë²„ì „)"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    # GPU ë©”ëª¨ë¦¬ë„ í™•ì¸
    gpu_memory = None
    try:
        import torch
        if torch.cuda.is_available():
            gpu_memory = {
                'allocated': torch.cuda.memory_allocated() / 1024 / 1024,  # MB
                'cached': torch.cuda.memory_reserved() / 1024 / 1024,  # MB
                'total': torch.cuda.get_device_properties(0).total_memory / 1024 / 1024  # MB
            }
    except:
        pass
    
    return {
        'rss': memory_info.rss / 1024 / 1024,  # MB
        'vms': memory_info.vms / 1024 / 1024,  # MB
        'percent': process.memory_percent(),
        'gpu': gpu_memory
    }


def optimize_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except:
        pass
    gc.collect()


def check_memory_health():
    """ë©”ëª¨ë¦¬ ìƒíƒœ ê±´ê°•ì„± ê²€ì‚¬"""
    memory_info = monitor_memory_usage()
    
    # CPU ë©”ëª¨ë¦¬ ê²€ì‚¬
    cpu_memory_gb = memory_info['rss'] / 1024
    if cpu_memory_gb > 4:  # 4GB ì´ìƒ
        print(f"âš ï¸  CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {cpu_memory_gb:.1f} GB")
        return False
    elif cpu_memory_gb > 2:  # 2GB ì´ìƒ
        print(f"âš ï¸  CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë³´í†µì…ë‹ˆë‹¤: {cpu_memory_gb:.1f} GB")
    else:
        print(f"âœ… CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ìƒ: {cpu_memory_gb:.1f} GB")
    
    # GPU ë©”ëª¨ë¦¬ ê²€ì‚¬
    if memory_info['gpu']:
        gpu_allocated_gb = memory_info['gpu']['allocated'] / 1024
        gpu_total_gb = memory_info['gpu']['total'] / 1024
        gpu_usage_percent = (memory_info['gpu']['allocated'] / memory_info['gpu']['total']) * 100
        
        if gpu_usage_percent > 80:
            print(f"âš ï¸  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {gpu_allocated_gb:.1f}/{gpu_total_gb:.1f} GB ({gpu_usage_percent:.1f}%)")
            return False
        elif gpu_usage_percent > 50:
            print(f"âš ï¸  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë³´í†µì…ë‹ˆë‹¤: {gpu_allocated_gb:.1f}/{gpu_total_gb:.1f} GB ({gpu_usage_percent:.1f}%)")
        else:
            print(f"âœ… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ìƒ: {gpu_allocated_gb:.1f}/{gpu_total_gb:.1f} GB ({gpu_usage_percent:.1f}%)")
    
    return True


def get_memory_usage_info():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ìƒì„¸íˆ ë°˜í™˜"""
    memory_info = monitor_memory_usage()
    
    info = {
        'cpu': {
            'rss_mb': memory_info['rss'],
            'vms_mb': memory_info['vms'],
            'percent': memory_info['percent']
        }
    }
    
    if memory_info['gpu']:
        info['gpu'] = memory_info['gpu']
    
    return info


def force_cleanup():
    """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    print("ğŸ§¹ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
    
    # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    collected = gc.collect()
    print(f"   Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´ ì •ë¦¬")
    
    # PyTorch GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   PyTorch GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    final_memory = monitor_memory_usage()
    print(f"   ì •ë¦¬ í›„ ë©”ëª¨ë¦¬: {final_memory['rss']:.1f} MB")
    
    return final_memory